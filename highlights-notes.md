# About preface

### 5 techniques of prompting:
* Zero-shot prompting
  * Just ask the question
* Chain of thought
  * Ask the question. Ask to think step by step
  * You can also write yourself the steps That helped in the boardgames chatbot.
* RAG
  * Include context in the prompt. 
* Tool calling
  * Adds to prompt a list of external functions and a description of what each does. 
  * Adds instructions on how to signal in the output that it wants to use one or some of these functions to better answer the question
  * The developer should parse the output and call the appropriate functions, and then returns the output of the functions to the model so it can provide the final answer to the user
* Few-shot prompting
  * Include in the prompt examples of other questions with the correct answers.
For best results, combine chain of thought, rag and tool calling, and even few-shot

### RAG terms:
Vector store: databases dedicated to storing embeddings
Vector indexes: regular databases with vector-storing capabilities


# Chapter 1

### Interfaces with LLMs
There are 2 interfaces with the llms:
* llms : input is just a string. Output is just a string.
* chat : allows to input conversation objects as input. Most useful for apps. The input is a list of messages (HumanMessage, SystemMessage, AIMessage). The output is an AIMessage
These 2 interfaces also exist for prompt templates. Template for llms interface is just an f-string. Template for chat interface is again a list of messages.

For chat interface models, you can use either **init_chat_model** or **ChatCohere**. However, **init_chat_model** covers less functionalities, so **ChatCohere** is adviced.

```scss
LangChain LLMs
├── Base LLMs (text generation)
│   ├── Cohere (langchain_cohere.Cohere)
│   └── OpenAI (langchain.OpenAI)
│
└── Chat Models (conversation-capable)
    ├── ChatCohere (langchain_cohere.ChatCohere)
    │   ├─ Supports messages: HumanMessage, SystemMessage
    │   ├─ Supports structured outputs: with_structured_output(PydanticModel)
    │   └─ Supports multi-turn chat
    │
    ├── ChatOpenAI (langchain.OpenAIChat)
    │   └─ OpenAI chat-specific features
    │
    └── Factory: init_chat_model (langchain.chat_models)
        ├─ Returns a chat model from any provider
        ├─ Provider-agnostic interface
        ├─ Structured outputs work if returned model supports it (chat-capable)
        └─ Simplifies switching between providers
```

Quick Notes:

* Base LLMs: Only generate text, no message history. with_structured_output usually does not work on them.
* Chat Models: Designed for multi-turn conversations, structured output parsing, and better context handling.
* init_chat_model: Convenience wrapper; internally returns the appropriate chat model for the provider.


### Messages Roles within the chat interface depending on the message object
* SystemMessage: System role. Instructions the model should use
* HumanMessage: User role. Content "produced" by the user 
* AIMessage: Assistant role. Content generated by the model 
* ChatMessage: message allowing for arbitrary role
* AIMessageChunk: Output of every chunk output when using the stream method.

**Note**: context should be given as human role, not system! (I saw drop in performance otherwise in the boardgames assistant)

### Specific format outputs
* You can force llms to retrieve a specific format output (json, csv, ...), with the help of pydantic.
* You cant use a init_chat_model model. You instead have to use ChatCohere


### Different methods for retrieving outputs
There are 3 methods to generate outputs for all different LangChain objects (models, templates, ...)
* invoke: transforms a single input into an output
* batch: efficiently transforms many inputs into many outputs
* stream: streams output from a single input as it is produced (outputs one token at a time)

Furthermore, **each of the 3 methods have asyncio equivalents!**

### Assembling LangChain objects
There are 2 ways to combine LangChain objects:
* Imperative: you just put together all steps inside a function, and add the @chain decorator to the function.
* Declarative: use "LangChain Expression Language" (LCEL)
The declarative syntax is super easy (you just add a "|") and it offers automatic parallel execution, streaming, and async execution. In the imperative syntax, you have to do that manually. So usually the declarative is easier and also better, but the imperative is useful if you have to write a lot of custom logic (the declarative simply chains existing components, with limited customization).
The cool thing about assembling objects, is that you can still use .invoke, .batch and .stream on the chain (kind of like sklearn pipelines that still have transform, predict and fit methods).


### Extra
* There is a **with_retry** option in the models that could be worth exploring. The book does not cover retries. There are also functionalities for fallbacks.



# Chapter 2

All this chapter is about the first part of RAG systems: indexing documents.

* A generative LLMs context window includes both input (the full prompt) and output. Be careful when the context is too long wrt the context window, because then the output of the model will be truncated. There is no straight way to map this to text splitters since context window is wrt tokens and splitters wrt characters, but a good approx is to assume that a token is between 3 and 4 letters long

### About data loaders
* There are different types of them, depending how the input is stored (.txt, .pdf, html...). There is even one that loads html text from a website!
* PyMuPDFLoader is better than PyPDFLoader. It is faster, and can extract more complex info from files such as coordinates, images, metadata

### About text splitters
* RecursiveCharacterTextSplitter splits first by paragraph, then by line, then by words. They do this on a hierarchichal way to guarantee all chunks are as big as possible, but semantically coherent.
* Splitters can be activated with these methods
  * split_documents : when inputs are documents objects
  * create_documents: when inputs are strings. This also allows passing data that you want to keep as metadata for each chunk.

### About embedding documents and vector stores
* Embed in a batch is more efficient for encoders (due to gpus probably i guess)
* Process to create vector stores from docker containers is explained well in the book
* More info on how to set up a vector store in qdrant, as well as how to manage it, is in the boardgames-assistant repo
* According to chatgpt, qdrant is the best open source vector store until now


### Strategies to improve search performance (Indexing optimization):
* MultiVectorRetriever: generate a summary of each chunk, and embed that summary. Perform similarity search using the embeddings of the summaries, but provide as context the full chunks instead of the summaries.
* RAPTOR: amazing solution to provide the RAG system with also highlevel understanding instead of low level (by that i mean knowledge accross chunks). 
  * Create summaries of each chunk. Embed those summaries and add them to the vector store
  * Cluster summary embeddings. For each cluster, create a summary of the summaries. Embed that summary, and add it to the vector store
  * Generate a summary of all the cluster summaries. Embed that summary, and add it to the vector store.


### Tracking document changes
Langchain comes with functionalities that make your system robust against changes in documents.
This is done with the RecordManager class. Basically it registers all chunks with their metadata. You define the metadata to use as key. Every time you sync the record manager, it will drop the previous chunks and embeddings related to the documents that were updated, and it will create new chunks and embeddings for these. More information and code under this section.


# Chapter 3
This chapter covers tools to make the retrieval of documents more accurate, consistent and robust. This is done with strategies like query transformation, query routing, smart filtering and query construction.

We also are shown that we can wrap the vector store as a retriever using ".as_retriever()" and then performing ".get_relevant_documents()", as a better alternative to ".similarity_search()" for production ready RAGs. I think that filters are easier to define there too (not sure though). Also, retriever wraps everything, even the decoder. Its the whole rag. However, I think that langgraph is a better approach, cleaner and with more visibility.

Fun fact: retrieving more documents (higher k) makes the RAG slower.

### Query transformation
Aims to solve the issue of users writing poor quality prompts. Two solutions are of interest:

* Rewrite-Retrieve-Read: Add an extra step that asks an llm to create a better prompt for the llm to answer the user. You could add to remove useless info for the llm too.
* Multi-Query Retrieval: Add an extra step that asks an llm to generate different prompts for the llm to answer the user. You then retrieve all chunks that were the most similar to each of the prompts, remove duplicate chunks, and add this as context for the llm to answer the original question.

I think approach 1 is better than 2. Approach 2 is an overkill unless the original prompt is asking for a lot of different things. All code is presented. 2 other approaches are presented but I did not see their advantage.

### Query routing
This one is very nice too. It aims to solve the issues when the document chunks are stored in multiple vector stores. 

* Local Routing: Using a LangChain wrapper of pydantic, you can add an extra step that detects, based on the user prompt, the best vector store for retrieving info. Thanks to pydantic you force the llm output to have the exact name of the vector store.  Nevertheless, this tool can be useful in way more cases: it can also be used to filter to specific chunks in the same vector store, for example.

In this section, we are also introduced briefly to how to add functions into a chain by wrapping them as a RunnableLambda (remember that all members of a chain need to have the methods .invoke, .batch, .stream)

* Semantic Routing: you create a description of each of the options for the routing, and you embed them. Then, to decide which routing to use for a given user prompt, you embed the user prompt and calculate its similarity with the options descriptions, and choose the route with the highest similarity. If you see it this way, in local routing you are letting a decoder decide which option to take, while in semantic routing you are letting the encoder do it.

I think semantic routing might yield better accuracy, but nowadays decoders are so strong that I will choose first local routing because it is easier to implement.


### Smart filtering
This part is called in the book "Text-to-Metadata" and it is written under the section "Query construction", but I thought it was better to make it apart. This tool is also very cool. Remember that in the boardgames assistant I created some filters that were quite hardcoded? Well, with LangChain, you can create filters based on the users input! The only requirement is that the information to be used as a filter should be part of the metadata of the chunks.

Under the hood, this approach first creates 2 outputs from the user prompt: the filters to applied, and a new user prompt without that info. Then it uses the output of the filters to actually create filters on the retrieval, and returns the chunks most similar to the new user prompt. Cool, right? You can even say "What are alien movies in 1980?", assuming the chunks are about movies and there is metadata with the year of the movie.

### Query construction
This is also very cool, and in the book is referred to "Text-to-SQL". This is mainly when you want to extract information from a relational database (e.g. any sql database) without using code (so in principle, its different than a typical RAG because we dont have to chunk documents and embed them, nor perform similarity search). In just a couple of lines of code, you can chat with your sql database asking things like "How many employees are there?"
You can then use an llm again to deliver the result of the database in text, although the code for that is not shown.


# Chapter 4

This chapter, called "Using LangGraph to Add Memory to Your Chatbot", gives an introduction to LangGraph and how it can be used to improve llm systems like adding flexibility, and memory to a user both within and accross sessions. We use langgraph in all chapters starting from this one.

The boardgames assistant is a good application of these concepts.


### Introducing LangGraph
It first covers the intuition of langgraph and shows that all nodes of a graph should input and output a state, which is a dictionary. Each node can modify the state by adding new keys or updating values of existing ones.

### Creating a StateGraph

Here we saw how to create a simple graph using a custom state. We also see the commands to make the graph always append new messages into their history.

We are also explained the syntax to create nodes (which is very simple)

When creating a graph (graph=builder.compile()), we create a graph with the familiar **invoke** and **stream** methods!

### Adding Memory to StateGraph

The state can be stored in a sql database! (called a checkpointer). Like that, you can always pick up your conversation when you left off!

In LangGraph, what would be a different conversation for different users will be called as "threads".

You can also inspect the state for a thread! Or manually update it!

### Modifying Chat History

We covered how to manipulate messages of a state in an efficient way.

For preventing memory from overflowing the context window (which is made of both input and output of an llm), we saw how to trim messages to a specific number of tokens. We think that a better alternative would be to trim messages to a specific number of tokens, but also generate a summary of the trimmed messages and include that in the context. That you probably have to do by hand though (probably LangChain does not have this).

Furthermore, you can filter to specific messages depending on their type (ai, system, human), but also on their message name, which is kind of like an optional metadata field for each message.









