# About preface

### 5 techniques of prompting:
* Zero-shot prompting
  * Just ask the question
* Chain of thought
  * Ask the question. Ask to think step by step
  * You can also write yourself the steps That helped in the boardgames chatbot.
* RAG
  * Include context in the prompt. 
* Tool calling
  * Adds to prompt a list of external functions and a description of what each does. 
  * Adds instructions on how to signal in the output that it wants to use one or some of these functions to better answer the question
  * The developer should parse the output and call the appropriate functions, and then returns the output of the functions to the model so it can provide the final answer to the user
* Few-shot prompting
  * Include in the prompt examples of other questions with the correct answers.
For best results, combine chain of thought, rag and tool calling, and even few-shot

### RAG terms:
Vector store: databases dedicated to storing embeddings
Vector indexes: regular databases with vector-storing capabilities


# Chapter 1

### Interfaces with LLMs
There are 2 interfaces with the llms:
* llms : input is just a string. Output is just a string.
* chat : allows to input conversation objects as input. Most useful for apps. The input is a list of messages (HumanMessage, SystemMessage, AIMessage). The output is an AIMessage
These 2 interfaces also exist for prompt templates. Template for llms interface is just an f-string. Template for chat interface is again a list of messages.

For chat interface models, you can use either **init_chat_model** or **ChatCohere**. However, **init_chat_model** covers less functionalities, so **ChatCohere** is adviced.

```scss
LangChain LLMs
â”œâ”€â”€ Base LLMs (text generation)
â”‚   â”œâ”€â”€ Cohere (langchain_cohere.Cohere)
â”‚   â””â”€â”€ OpenAI (langchain.OpenAI)
â”‚
â””â”€â”€ Chat Models (conversation-capable)
    â”œâ”€â”€ ChatCohere (langchain_cohere.ChatCohere)
    â”‚   â”œâ”€ Supports messages: HumanMessage, SystemMessage
    â”‚   â”œâ”€ Supports structured outputs: with_structured_output(PydanticModel)
    â”‚   â””â”€ Supports multi-turn chat
    â”‚
    â”œâ”€â”€ ChatOpenAI (langchain.OpenAIChat)
    â”‚   â””â”€ OpenAI chat-specific features
    â”‚
    â””â”€â”€ Factory: init_chat_model (langchain.chat_models)
        â”œâ”€ Returns a chat model from any provider
        â”œâ”€ Provider-agnostic interface
        â”œâ”€ Structured outputs work if returned model supports it (chat-capable)
        â””â”€ Simplifies switching between providers
```

Quick Notes:

* Base LLMs: Only generate text, no message history. with_structured_output usually does not work on them.
* Chat Models: Designed for multi-turn conversations, structured output parsing, and better context handling.
* init_chat_model: Convenience wrapper; internally returns the appropriate chat model for the provider.


### Messages Roles within the chat interface depending on the message object
* SystemMessage: System role. Instructions the model should use
* HumanMessage: User role. Content "produced" by the user 
* AIMessage: Assistant role. Content generated by the model 
* ChatMessage: message allowing for arbitrary role
* AIMessageChunk: Output of every chunk output when using the stream method.

**Note**: context should be given as human role, not system! (I saw drop in performance otherwise in the boardgames assistant)

### Specific format outputs
* You can force llms to retrieve a specific format output (json, csv, ...), with the help of pydantic.
* You cant use a init_chat_model model. You instead have to use ChatCohere


### Different methods for retrieving outputs
There are 3 methods to generate outputs for all different LangChain objects (models, templates, ...)
* invoke: transforms a single input into an output
* batch: efficiently transforms many inputs into many outputs
* stream: streams output from a single input as it is produced (outputs one token at a time)

Furthermore, **each of the 3 methods have asyncio equivalents!**

### Assembling LangChain objects
There are 2 ways to combine LangChain objects:
* Imperative: you just put together all steps inside a function, and add the @chain decorator to the function.
* Declarative: use "LangChain Expression Language" (LCEL)
The declarative syntax is super easy (you just add a "|") and it offers automatic parallel execution, streaming, and async execution. In the imperative syntax, you have to do that manually. So usually the declarative is easier and also better, but the imperative is useful if you have to write a lot of custom logic (the declarative simply chains existing components, with limited customization).
The cool thing about assembling objects, is that you can still use .invoke, .batch and .stream on the chain (kind of like sklearn pipelines that still have transform, predict and fit methods).


### Extra
* There is a **with_retry** option in the models that could be worth exploring. The book does not cover retries. There are also functionalities for fallbacks.



# Chapter 2

All this chapter is about the first part of RAG systems: indexing documents.

* A generative LLMs context window includes both input (the full prompt) and output. Be careful when the context is too long wrt the context window, because then the output of the model will be truncated. There is no straight way to map this to text splitters since context window is wrt tokens and splitters wrt characters, but a good approx is to assume that a token is between 3 and 4 letters long

### About data loaders
* There are different types of them, depending how the input is stored (.txt, .pdf, html...). There is even one that loads html text from a website!
* PyMuPDFLoader is better than PyPDFLoader. It is faster, and can extract more complex info from files such as coordinates, images, metadata

### About text splitters
* RecursiveCharacterTextSplitter splits first by paragraph, then by line, then by words. They do this on a hierarchichal way to guarantee all chunks are as big as possible, but semantically coherent.
* They also have the option to split semantically correctly python code or more importantle markdown (for documentation).
* Splitters can be activated with these methods
  * split_documents : when inputs are documents objects
  * create_documents: when inputs are strings. This also allows passing data that you want to keep as metadata for each chunk.

### About embedding documents and vector stores
* Embed in a batch is more efficient for encoders (due to gpus probably i guess)
* Process to create vector stores from docker containers is explained well in the book
* More info on how to set up a vector store in qdrant, as well as how to manage it, is in the boardgames-assistant repo
* According to chatgpt, qdrant is the best open source vector store until now


### Strategies to improve search performance (Indexing optimization):
* MultiVectorRetriever: generate a summary of each chunk, and embed that summary. Perform similarity search using the embeddings of the summaries, but provide as context the full chunks instead of the summaries.
* RAPTOR: amazing solution to provide the RAG system with also highlevel understanding instead of low level (by that i mean knowledge accross chunks). 
  * Create summaries of each chunk. Embed those summaries and add them to the vector store
  * Cluster summary embeddings. For each cluster, create a summary of the summaries. Embed that summary, and add it to the vector store
  * Generate a summary of all the cluster summaries. Embed that summary, and add it to the vector store.


### Tracking document changes
Langchain comes with functionalities that make your system robust against changes in documents.
This is done with the RecordManager class. Basically it registers all chunks with their metadata. You define the metadata to use as key. Every time you sync the record manager, it will drop the previous chunks and embeddings related to the documents that were updated, and it will create new chunks and embeddings for these. More information and code under this section.


# Chapter 3
This chapter covers tools to make the retrieval of documents more accurate, consistent and robust. This is done with strategies like query transformation, query routing, smart filtering and query construction.

We also are shown that we can wrap the vector store as a retriever using ".as_retriever()" and then performing "invoke" or ".get_relevant_documents()", as a better alternative to ".similarity_search()" for production ready RAGs. This is because every time we want to do a vector similarity with a query, we just have to run ".invoke('this is the query')", instead of first embedding the query and then doing vector similarity. Filters can also be applied there, although in Qdrant also with similarity_searh. I still think that langgraph could a better approach sometimes?, cleaner and with more visibility?

Fun fact: retrieving more documents (higher k) makes the RAG slower.

### Query transformation
Aims to solve the issue of users writing poor quality prompts. Two solutions are of interest:

* Rewrite-Retrieve-Read: Add an extra step that asks an llm to create a better prompt for the llm to answer the user. You could add to remove useless info for the llm too.
* Multi-Query Retrieval: Add an extra step that asks an llm to generate different prompts for the llm to answer the user. You then retrieve all chunks that were the most similar to each of the prompts, remove duplicate chunks, and add this as context for the llm to answer the original question.

I think approach 1 is better than 2. Approach 2 is an overkill unless the original prompt is asking for a lot of different things. All code is presented. 2 other approaches are presented but I did not see their advantage.

### Query routing
This one is very nice too. It aims to solve the issues when the document chunks are stored in multiple vector stores. 

* Local Routing: Using a LangChain wrapper of pydantic, you can add an extra step that detects, based on the user prompt, the best vector store for retrieving info. Thanks to pydantic you force the llm output to have the exact name of the vector store.  Nevertheless, this tool can be useful in way more cases: it can also be used to filter to specific chunks in the same vector store, for example.

In this section, we are also introduced briefly to how to add functions into a chain by wrapping them as a RunnableLambda (remember that all members of a chain need to have the methods .invoke, .batch, .stream)

* Semantic Routing: you create a description of each of the options for the routing, and you embed them. Then, to decide which routing to use for a given user prompt, you embed the user prompt and calculate its similarity with the options descriptions, and choose the route with the highest similarity. If you see it this way, in local routing you are letting a decoder decide which option to take, while in semantic routing you are letting the encoder do it.

I think semantic routing might yield better accuracy, but nowadays decoders are so strong that I will choose first local routing because it is easier to implement.


### Smart filtering
This part is called in the book "Text-to-Metadata" and it is written under the section "Query construction", but I thought it was better to make it apart. This tool is also very cool. Remember that in the boardgames assistant I created some filters that were quite hardcoded? Well, with LangChain, you can create filters based on the users input! The only requirement is that the information to be used as a filter should be part of the metadata of the chunks.

Under the hood, this approach first creates 2 outputs from the user prompt: the filters to applied, and a new user prompt without that info. Then it uses the output of the filters to actually create filters on the retrieval, and returns the chunks most similar to the new user prompt. Cool, right? You can even say "What are alien movies in 1980?", assuming the chunks are about movies and there is metadata with the year of the movie.

### Query construction
This is also very cool, and in the book is referred to "Text-to-SQL". This is mainly when you want to extract information from a relational database (e.g. any sql database) without using code (so in principle, its different than a typical RAG because we dont have to chunk documents and embed them, nor perform similarity search). In just a couple of lines of code, you can chat with your sql database asking things like "How many employees are there?"
You can then use an llm again to deliver the result of the database in text, although the code for that is not shown.


# Chapter 4

This chapter, called "Using LangGraph to Add Memory to Your Chatbot", gives an introduction to LangGraph and how it can be used to improve llm systems like adding flexibility, and memory to a user both within and accross sessions. We use langgraph in all chapters starting from this one.

The boardgames assistant is a good application of these concepts.


### Introducing LangGraph
It first covers the intuition of langgraph and shows that all nodes of a graph should input and output a state, which is a dictionary. Each node can modify the state by adding new keys or updating values of existing ones.

### Creating a StateGraph

Here we saw how to create a simple graph using a custom state. We also see the commands to make the graph always append new messages into their history.

We are also explained the syntax to create nodes (which is very simple)

When creating a graph (graph=builder.compile()), we create a graph with the familiar **invoke** and **stream** methods!

### Adding Memory to StateGraph

The state can be stored in a sql database! (called a checkpointer). Like that, you can always pick up your conversation when you left off!

In LangGraph, what would be a different conversation for different users will be called as "threads".

You can also inspect the state for a thread! Or manually update it!

### Modifying Chat History

We covered how to manipulate messages of a state in an efficient way.

For preventing memory from overflowing the context window (which is made of both input and output of an llm), we saw how to trim messages to a specific number of tokens. We think that a better alternative would be to trim messages to a specific number of tokens, but also generate a summary of the trimmed messages and include that in the context. That you probably have to do by hand though (probably LangChain does not have this).

Furthermore, you can filter to specific messages depending on their type (ai, system, human), but also on their message name, which is kind of like an optional metadata field for each message.


# Chapter 5

This chapter, called "Cognitive Architectures with LangGraph", goes through different levels of LLMs automation, providing examples of each of them.
There is always a balance that needs to be taken into account in LLM applications. The more autonomous an LLM application is, the more it can do- but the more prone is to make mistakes. This is also called balance between agency and reliability.

In this chapter we saw from the simplest level, which is just a single LLM call, to a level which uses conditional edges, and an LLM output decides to which edge to go now. Agents will be for next chapter.

It would be super exciting to work on the example mentioned in the first page. Might be a good side project.

* We saw that a good practice for LLM apps with multiple nodes that call an LLM is to exclude the system message from the messages history. The reasoning of this is that while the messages history is used by all nodes, the system message depends on the node. 

* We saw that you can mask parts of the state so that only particular keys are output to the user. You can also define a different state key to store the user query. These masks are also defined as dictionaries, with a subset of the keys of the state.

* It is very useful to look the scripts of architecture #2 and #3. In #3 we make use of conditional edges.

* Using the function "as retriever" really reduces the code a lot when using RAG (until vector similarity, then you have to manually summarize the most similar documents.)

* We saw that the code to create conditional edges is super easy, even easier than the one I implemented in the boardgames assistant.

* We saw how adding different keys into the State dictionary can lead to more flexibility in your app. For example, adding a key for the RAG documents extracted to also output that info (or for monitoring performance), saving the user query in a different key to be used in different nodes, and auxiliar keys that help define the routing when using conditional edges.

# Chapter 6
High level stuff
* This chapter is very very useful. Give it a read whenever you want to implement agents. 
* A tool can be any function (python logic, api call...) that could be useful in providing a better answer to the user, than using the llm alone.
* An agent is a langgraph graph that has the caoabilities to choose which tools to use and applies them.
* The ability of an llm to use tools relies solely on prompt engineering. The first part of this chapter shows this with a very nice example.
* The agent focuses only on one step at a time, and chooses the best tool to use as it goes.
* Its not difficult to define an agent graph.
* There is one node containing all tools in the graph, not one node per tool.
* You can create custom tools using the @tool decorator.
* An agent performs better, or hallucinates less, if the number of tools it has to decide on is not too long. For more than 10 tools, it is advised to first select a subset of tools based on vector similarity between the tools descriptions and the user query, and provide only these tools to the llm.

Low level stuff:
* In an agent, you dont have to define the stop node. That logic is also given as part of the function "bind_tools".
* You dont have to create the prompt templates manually. That happens via the method "bind_tools". This method is supported by all commercial llms that langchain offers as chat models.
* You can also call an tool manually. For this, you need to output a tool call message with the name of the tool. The name of the tool can be found in its ".name" attribute.
* Under the hood, the llm is given the user query and a list of tools to use to solve it. The llm decides if it needs a tool or not. If it does not need a tool, it just outputs the answer to the query. If it decides it needs to call a tool, it will output a ToolCall message, with the name of the tool to call, and the body of the tool (All of this happens due to smart prompt engineering that gave specific instructions on tool usage to the llm. What is very very important too, is to have self-explanatory tool names and a very good description of the tool. This is because this is also included in the prompt engineering.)
Thanks to the function "tools_condition" (that probably under the hood says, if the last message is a Tool call, then return the name of the node where your tools are), when the llm needs to run a tool, the next step in the graph is to run such tool and append its output in the messages history. After that, we go back to the model (we add an edge between tools and model), and the model again can decide if it contains all info needed to provide an answer, or if it needs to call another tool. Again, the magic lies in good prompt engineering.
* The node for the tools needs to be the list of tools objects wrapped in the ToolNode() function.



# Chapter 7

In this chapter, called **Agents 2**, we saw more complex additions to the agents we saw in the previous chapter.

### Reflection

This section is very cool because it shows you how to make use of agents to retrieve more polished answers. This is done by creating 2 tools, one that will generate the output for the user, and one that will evaluate such output and provide recommendations on how to improve it.

The implementation is tricky so its a good idea to look at the code and my notes:
* For the generator, you have to make the generator think that the recommendations were done by a human.
  * This is solved by outputing the evaluator reccomendations as HumanMessage
* For the evaluator, you have to make the evaluator think that the generated text was done by a human
  * This is solved by temporarily inverting the messages types from Human to AI and vice versa. 
* The previous points are needed because some chat models dont accept 2 messages in a row from the same type, or they will accept it but it will yield poor performance.

This approach is highly recommended by the authors.

In this approach, it is again better to not append the system messages to the chain of messages, given that different system messages exist.

The approach is very cool, it kind of mimicks a student-teacher relationship between evaluator and generator.

Reflection is technically reffered to be a prompting technique.


### Subgraphs in LangGraph

This section shows you that you can use graphs as nodes in a bigger graph. This is the base of multi-agent system architectures. You literally just have to declare a graph as a node.

For this to work, all subgraphs need to have the same keys as the parent state. If there are keys that are not present in the parent state, they will be ignored. In case the keys of your subgraph are different, you can make a function that calls the graph with renamed keys, and use that as a node instead of the graph itself.


### Multi-Agent Architectures

Multi agent architectures are graphs where several nodes are agents (remember that an agent is simply a graph that has access to tools and can decide which tools to use and when, thanks to smart prompt engineering. Since an agent is a graph, and graphs can be run as nodes, then agents can be run as nodes). They showed a vanilla use case here since the topic is already complex, and they recommend first doing a single agent architecture, and only when needed turn to multi agent ones.

There are different architectures. We saw the Supervisor one, where an agent decides which agent shall run next.


# Chapter 8

This covers some good practices to make the most of the llms in an app.

### Structured output -under the hood-
Different llms use different strategies to force the llm to give the output in a specific format. Instead of learning the different ways each model does that, you can use structured_output, a langchain common interface for that.

It is very, very important to add a good description to each of the fields you want to retrieve. This will be the information that the llm will use to decide what to output where.

This approach is not bulletproof, and the llm can still return a format that did not adhere to the expected one. If that happens, there will occur an error since we used pydantic to make the schema. This means you have to make the app robust against these scenarios.

A way to minimize the chances of getting schema errors is to set the temperature low.


### Streaming outputs
Good practices: streaming outputs. Otherwise users get desperate.

There are 2 ways you can do this. The first one streams stuff after each node is run. This is called here as "Intermediate Output". The second one streams stuff inside nodes, token by token. This is called here as "Streaming token by token". The first one might be useful for the developer for debugging, and the second one for production settings.

**Note:**  for doing this you need to use async programming:

In LangGraph, using async is necessary for streaming outputs because streaming inherently relies on asynchronous operations. Here's a breakdown of why this is the case:

ðŸ”„ Streaming = Non-blocking Communication

When you stream outputs, you're:

Sending partial results (e.g. tokens, messages) incrementally as they are generated.

Not waiting for the entire response to be ready before sending it.

To do this efficiently, you need non-blocking I/O, which means the program can continue running other tasks while waiting for the next chunk of data

### Humans intervention during a graph run
We saw how to let the user humans take some decisions on the graph. This is done again with functions from the async package. You also need to provide your graph with memory.

The scripts shown in this part are just a fragment of the code needed to apply this. To see all the script, go to the repository.

For this section, you need a decent knowledge of async package. I wrote some info on the notes and that should be enough for this part, but feel free to check the youtube tutorial on async functions.

##### Modes for humans to intervene in the graph
For the end user:
* You can let the end user stop the outputs generated 
* You can let the user decide if the tool suggested by the llm shall be used or not

For the developer:
* You can authorize a graph to access a specific tool or not

You can determine the behaviour of the graph in case the user decides to write a new query before the output of the previous query is done. Some of these behaviours are:
 * Dont let the user do that (not great)
 * Let the user do it, but continue to stream the output of the previous query. Stream the output of the new query after the previous one is finished (not great)
 * Interrupt the streaming of the previous. Start right away with the streaming of the new query. (preferred)

Other cool stuff we saw:
* You can edit the current state
* You can even access the history of the state!!! kind of like github commits! That is because under the hood, when you provide the graph with memory, the graph stores the status of the state after each node is run. So you have complete lineage of the transformation of the state.

# Chapter 9

At the beginning I wonder how will this chapter is different from a traditional deployment like the one I did in the boardgames assistant. I realize the difference was they are using a serverless hosting that is paid (LangGraph Platform). Still a couple of sections of this chapter are useful. 

### About "Prerequisites" section:

We saw how to set a vector store in the cloud and access it via a url and a key. Even though the example used Supabase, you can do exactly the same in Qdrant.

At work, you can also always use qdrant inside aws or azure, and the approach is very similar to what we learned in the ml engineering book. That is, deploy qdrant as a container via any of the possible alternatives (AWS EC2 (vm), ECS (containers), EKS (k8))



### About "Understanding the LangGraph Platform API" section:

Good if you want to use LangGraph Platform, but since it is serverless and you have to pay, its not great for my learning objectives.

### About "Deploying Your AI App on LangGraph Platform" section:

This chapter was supposed to show you how to deploy your graph, but they only did it via LangGraph Platform, a serverless host embedded in LangSmith but that comes with a price... The deployment is super easy, like in Render. You need to setup a langgraph config file (which includes a pointer to the graph you want to deploy) and run the langgraph dev command in the same location as your project. 

Good news is, LangSmith, the software used to track and monitor threads, is free, and you can use it for everything, not just LangGraph graphs. Even by a non-LLM machine learning model! This chapter also shows you how to create a langsmith account and use it. This is also done in the **Prerequisites** section, and a tiny bit in this section, in the LangSmith subsection.

### About "Security" section:

The section about security is a useful one. It gives some tips on how to improve it. The most important ones that I remember were:
* In the matter of possible give only **read-only** privileges to the agents
* Set guardrails in case people want to change the purpose of the system (by making the agent think the instructions of the system changed)
* Restric the databases to be reach to the agent to only  the ones you want and nothing more
* Make sure each thread is identified with a real person, preferrably via a login
* Dont let a single user send too many queries in a very short time (increases cost)



# Chapter 10

This chapter, which is called "Testing: Evaluation, Monitoring and Continuous Improvement" is very very useful. It marks the difference between very professional, and just professional ML engineers.  It is about how to test the LLM system performance, and how to add robustness to guarantee a good performance.

There are 3 stages throughout the LLM system lifecycle:
* Design
* Preproduction
* Production
These 3 do not overlap, and they serve different uses (you need to implement all 3 of them). In each of them, different strategies can be used to improve the system robustness and test its performance.

### Stage 1: Design

In this stage, we can add robustness to our system by using a design robust against hallucinations. This is done by incorporating in the LangGraph graph, nodes that try to detect if the output of previous nodes makes sense or not. In case it does not make sense, it does not provide the output to the user, but instead it tries to generate a reasonable answer using alternative nodes.

The example provided in the book is about a rag system. After calculating vector similarity between the user query and the vector store, a first robustness node evaluates if the answer to the user query can be found in the documents or not. Assuming it does, the rag system generates an answer to the user. But before giving it to the user, another robustness node evaluates if the answer is correct (checks for hallucination). Only after that, is the output provided to the user.

Sample code of this is given in the script:
https://github.com/CarlosIvan9/learning-langchain-book/blob/master/ch10/py/search_graph.py


### Stage 2: Preproduction

This stage focuses on measuring performance of the system. By this, we mean to generate a numerical metric that can say how good the system is. It is mainly to benchmark the model, and this will also be useful for comparing different systems. The difference between benchmarking a traditional ML system and a LLM system is that not only the final output is measured in the LLM system, but also intermediate steps. 

So we measure performance by assessing accuracy of:
* The system final output
* The output of specific nodes
* The trajectory of the system (specially useful for Agents)
You can consider the metrics of all these approaches to benchmark system against each other and decide which one is better, just like in traditional ML.

We do all of this in a benchmarking dataset we create. This dataset must contain ground truths of the different types we want to assess performance. The dataset does not have to be too big. Even 30 user queries is fine, and later we could make a better version of it by adding edge cases we see after deployment. 

The dataset can be created in LangSmith. This section ellaborates more on its advantages and how to do it.

Langchain provides very nice functionality to try to make this as automated as possible, and with LangSmith you can visuallize each of the nodes run in a run, as well as their inputs and outputs. Finally, you can also add this in the ci/cd pipeline as part of ci.

The scripts used for this part are:
https://github.com/CarlosIvan9/learning-langchain-book/blob/master/ch10/py/agent_evaluation_sql.py

And for creating datasets:
https://github.com/CarlosIvan9/learning-langchain-book/blob/master/ch10/py/create_sql_dataset.py

##### Assessing accuracy of final output
This is literally just comparing the final output of the system against the final ground truth. We use an LLM as an evaluator to say if the generated output answers the question as good as the ground truth or not.

An idea I came up with is to generate different levels of ground truth, one that would be a great answer, but also a good answer, an okay-ish answer and a bad answer. I can then ask the LLM to rank these 4 ground truths + the generated output in terms of how well they answer the question. Depending the position of the generated output, is the score it will receive (0, 0.25, 0.5, 0.75, 1). I think this approach is brilliant, but it takes more time to prepare the benchmarking dataset.

##### Assessing accuracy of outputs of specific nodes
Similar approach as with the final output, with the difference that the output is from a node. We have to have ground truths of nodes outputs, and the input we feed is not the user query but the input the node receives.

##### Assessing accuracy of the system's trajectory
This is useful for example if we want to assess if the system is using the tools correctly or not. You need trajectory ground truths. You can assess trajectory accuracy in different ways. 
* Wheter the trajectory was exactly like the ground truth or not
* Wheter the trajectory included some of the trajectory of the ground truth
* Wheter the trajectory included all of the trajectory of the ground truth, even though extra steps were added
Trajectory is an ordered sequence of steps.

### Stage 3: Production
In this stage we try to monitor and measure performance of the system with served data. 

Before deployment, first deploy the solution but only make it accessible to a few users, which will provide you with feedback. You can also ask the users to write what they would like to receive as an answer, and in this way you can add their queries into the benchmark dataset. Also if some edge cases where found that the model does not behave well, add those to the dataset.

After that, deploy the system, and if possible ask to give feedback of the answers with a thumbs up or down (like in chatgpt). Using LangSmith, monitor the user queries and these feedbacks.


# Chapter 11
I did not find anything to be very relevant.
