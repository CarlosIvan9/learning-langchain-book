# About preface

### 5 techniques of prompting:
* Zero-shot prompting
  * Just ask the question
* Chain of thought
  * Ask the question. Ask to think step by step
  * You can also write yourself the steps That helped in the boardgames chatbot.
* RAG
  * Include context in the prompt. 
* Tool calling
  * Adds to prompt a list of external functions and a description of what each does. 
  * Adds instructions on how to signal in the output that it wants to use one or some of these functions to better answer the question
  * The developer should parse the output and call the appropriate functions, and then returns the output of the functions to the model so it can provide the final answer to the user
* Few-shot prompting
  * Include in the prompt examples of other questions with the correct answers.
For best results, combine chain of thought, rag and tool calling, and even few-shot

### RAG terms:
Vector store: databases dedicated to storing embeddings
Vector indexes: regular databases with vector-storing capabilities


# Chapter 1

### Interfaces with LLMs
There are 2 interfaces with the llms:
* llms : input is just a string. Output is just a string.
* chat : allows to input conversation objects as input. Most useful for apps. The input is a list of messages (HumanMessage, SystemMessage, AIMessage). The output is an AIMessage
These 2 interfaces also exist for prompt templates. Template for llms interface is just an f-string. Template for chat interface is again a list of messages.

For chat interface models, you can use either **init_chat_model** or **ChatCohere**. However, **init_chat_model** covers less functionalities, so **ChatCohere** is adviced.

```scss
LangChain LLMs
├── Base LLMs (text generation)
│   ├── Cohere (langchain_cohere.Cohere)
│   └── OpenAI (langchain.OpenAI)
│
└── Chat Models (conversation-capable)
    ├── ChatCohere (langchain_cohere.ChatCohere)
    │   ├─ Supports messages: HumanMessage, SystemMessage
    │   ├─ Supports structured outputs: with_structured_output(PydanticModel)
    │   └─ Supports multi-turn chat
    │
    ├── ChatOpenAI (langchain.OpenAIChat)
    │   └─ OpenAI chat-specific features
    │
    └── Factory: init_chat_model (langchain.chat_models)
        ├─ Returns a chat model from any provider
        ├─ Provider-agnostic interface
        ├─ Structured outputs work if returned model supports it (chat-capable)
        └─ Simplifies switching between providers
```

Quick Notes:

* Base LLMs: Only generate text, no message history. with_structured_output usually does not work on them.
* Chat Models: Designed for multi-turn conversations, structured output parsing, and better context handling.
* init_chat_model: Convenience wrapper; internally returns the appropriate chat model for the provider.


### Messages Roles within the chat interface depending on the message object
* SystemMessage: System role. Instructions the model should use
* HumanMessage: User role. Content "produced" by the user 
* AIMessage: Assistant role. Content generated by the model 
* ChatMessage: message allowing for arbitrary role
* AIMessageChunk: Output of every chunk output when using the stream method.

**Note**: context should be given as human role, not system! (I saw drop in performance otherwise in the boardgames assistant)

### Specific format outputs
* You can force llms to retrieve a specific format output (json, csv, ...), with the help of pydantic.
* You cant use a init_chat_model model. You instead have to use ChatCohere


### Different methods for retrieving outputs
There are 3 methods to generate outputs for all different LangChain objects (models, templates, ...)
* invoke: transforms a single input into an output
* batch: efficiently transforms many inputs into many outputs
* stream: streams output from a single input as it is produced (outputs one token at a time)

Furthermore, **each of the 3 methods have asyncio equivalents!**

### Assembling LangChain objects
There are 2 ways to combine LangChain objects:
* Imperative: you just put together all steps inside a function, and add the @chain decorator to the function.
* Declarative: use "LangChain Expression Language" (LCEL)
The declarative syntax is super easy (you just add a "|") and it offers automatic parallel execution, streaming, and async execution. In the imperative syntax, you have to do that manually. So usually the declarative is easier and also better, but the imperative is useful if you have to write a lot of custom logic (the declarative simply chains existing components, with limited customization).
The cool thing about assembling objects, is that you can still use .invoke, .batch and .stream on the chain (kind of like sklearn pipelines that still have transform, predict and fit methods).


### Extra
* There is a **with_retry** option in the models that could be worth exploring. The book does not cover retries. There are also functionalities for fallbacks.



# Chapter 2

All this chapter is about the first part of RAG systems: indexing documents.

* A generative LLMs context window includes both input (the full prompt) and output. Be careful when the context is too long wrt the context window, because then the output of the model will be truncated. There is no straight way to map this to text splitters since context window is wrt tokens and splitters wrt characters, but a good approx is to assume that a token is between 3 and 4 letters long

### About data loaders
* There are different types of them, depending how the input is stored (.txt, .pdf, html...). There is even one that loads html text from a website!
* PyMuPDFLoader is better than PyPDFLoader. It is faster, and can extract more complex info from files such as coordinates, images, metadata

### About text splitters
* RecursiveCharacterTextSplitter splits first by paragraph, then by line, then by words. They do this on a hierarchichal way to guarantee all chunks are as big as possible, but semantically coherent.
* Splitters can be activated with these methods
  * split_documents : when inputs are documents objects
  * create_documents: when inputs are strings. This also allows passing data that you want to keep as metadata for each chunk.

### About embedding documents and vector stores
* Embed in a batch is more efficient for encoders (due to gpus probably i guess)
* Process to create vector stores from docker containers is explained well in the book
* More info on how to set up a vector store in qdrant, as well as how to manage it, is in the boardgames-assistant repo
* According to chatgpt, qdrant is the best open source vector store until now


### Strategies to improve search performance (Indexing optimization):
* MultiVectorRetriever: generate a summary of each chunk, and embed that summary. Perform similarity search using the embeddings of the summaries, but provide as context the full chunks instead of the summaries.
* RAPTOR: amazing solution to provide the RAG system with also highlevel understanding instead of low level (by that i mean knowledge accross chunks). 
  * Create summaries of each chunk. Embed those summaries and add them to the vector store
  * Cluster summary embeddings. For each cluster, create a summary of the summaries. Embed that summary, and add it to the vector store
  * Generate a summary of all the cluster summaries. Embed that summary, and add it to the vector store.


### Tacking document changes
Langchain comes with functionalities that make your system robust against changes in documents.
This is done with the RecordManager class. Basically it registers all chunks with their metadata. You define the metadata to use as key. Every time you sync the record manager, it will drop the previous chunks and embeddings related to the documents that were updated, and it will create new chunks and embeddings for these. More information and code under this section.


