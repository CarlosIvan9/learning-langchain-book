# About preface

### 5 techniques of prompting:
* Zero-shot prompting
  * Just ask the question
* Chain of thought
  * Ask the question. Ask to think step by step
  * You can also write yourself the steps That helped in the boardgames chatbot.
* RAG
  * Include context in the prompt. 
* Tool calling
  * Adds to prompt a list of external functions and a description of what each does. 
  * Adds instructions on how to signal in the output that it wants to use one or some of these functions to better answer the question
  * The developer should parse the output and call the appropriate functions, and then returns the output of the functions to the model so it can provide the final answer to the user
* Few-shot prompting
  * Include in the prompt examples of other questions with the correct answers.
For best results, combine chain of thought, rag and tool calling, and even few-shot

### RAG terms:
Vector store: databases dedicated to storing embeddings
Vector indexes: regular databases with vector-storing capabilities


# Chapter 1

### Interfaces with LLMs
There are 2 interfaces with the llms:
* llms : input is just a string. Output is just a string.
* chat : allows to input conversation objects as input. Most useful for apps. The input is a list of messages (HumanMessage, SystemMessage, AIMessage). The output is an AIMessage
These 2 interfaces also exist for prompt templates. Template for llms interface is just an f-string. Template for chat interface is again a list of messages.

```scss
LangChain LLMs
├── Base LLMs (text generation)
│   ├── Cohere (langchain_cohere.Cohere)
│   └── OpenAI (langchain.OpenAI)
│
└── Chat Models (conversation-capable)
    ├── ChatCohere (langchain_cohere.ChatCohere)
    │   ├─ Supports messages: HumanMessage, SystemMessage
    │   ├─ Supports structured outputs: with_structured_output(PydanticModel)
    │   └─ Supports multi-turn chat
    │
    ├── ChatOpenAI (langchain.OpenAIChat)
    │   └─ OpenAI chat-specific features
    │
    └── Factory: init_chat_model (langchain.chat_models)
        ├─ Returns a chat model from any provider
        ├─ Provider-agnostic interface
        ├─ Structured outputs work if returned model supports it (chat-capable)
        └─ Simplifies switching between providers
```

Quick Notes:

* Base LLMs: Only generate text, no message history. with_structured_output usually does not work on them.
* Chat Models: Designed for multi-turn conversations, structured output parsing, and better context handling.
* init_chat_model: Convenience wrapper; internally returns the appropriate chat model for the provider.


### Messages Roles within the chat interface depending on the message object
* SystemMessage: System role. Instructions the model should use
* HumanMessage: User role. Content "produced" by the user 
* AIMessage: Assistant role. Content generated by the model 
* ChatMessage: message allowing for arbitrary role

**Note**: context should be given as human role, not system! (I saw drop in performance otherwise in the boardgames assistant)

### Specific format outputs
* You can force llms to retrieve a specific format output (json, csv, ...), with the help of pydantic.
* You cant use a init_chat_model model. You instead have to use ChatCohere

### with_retry
* There is a with_retry option in the models that could be worth exploring. The book does not cover retries
