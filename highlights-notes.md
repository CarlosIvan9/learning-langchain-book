# About preface

### 5 techniques of prompting:
* Zero-shot prompting
  * Just ask the question
* Chain of thought
  * Ask the question. Ask to think step by step
  * You can also write yourself the steps That helped in the boardgames chatbot.
* RAG
  * Include context in the prompt. 
* Tool calling
  * Adds to prompt a list of external functions and a description of what each does. 
  * Adds instructions on how to signal in the output that it wants to use one or some of these functions to better answer the question
  * The developer should parse the output and call the appropriate functions, and then returns the output of the functions to the model so it can provide the final answer to the user
* Few-shot prompting
  * Include in the prompt examples of other questions with the correct answers.
For best results, combine chain of thought, rag and tool calling, and even few-shot

### RAG terms:
Vector store: databases dedicated to storing embeddings
Vector indexes: regular databases with vector-storing capabilities


# Chapter 1

### Interfaces with LLMs
There are 2 interfaces with the llms:
* llms : input is just a string. Output is just a string.
* chat : allows to input conversation objects as input. Most useful for apps. The input is a list of messages (HumanMessage, SystemMessage, AIMessage). The output is an AIMessage
These 2 interfaces also exist for prompt templates. Template for llms interface is just an f-string. Template for chat interface is again a list of messages.

For chat interface models, you can use either **init_chat_model** or **ChatCohere**. However, **init_chat_model** covers less functionalities, so **ChatCohere** is adviced.

```scss
LangChain LLMs
├── Base LLMs (text generation)
│   ├── Cohere (langchain_cohere.Cohere)
│   └── OpenAI (langchain.OpenAI)
│
└── Chat Models (conversation-capable)
    ├── ChatCohere (langchain_cohere.ChatCohere)
    │   ├─ Supports messages: HumanMessage, SystemMessage
    │   ├─ Supports structured outputs: with_structured_output(PydanticModel)
    │   └─ Supports multi-turn chat
    │
    ├── ChatOpenAI (langchain.OpenAIChat)
    │   └─ OpenAI chat-specific features
    │
    └── Factory: init_chat_model (langchain.chat_models)
        ├─ Returns a chat model from any provider
        ├─ Provider-agnostic interface
        ├─ Structured outputs work if returned model supports it (chat-capable)
        └─ Simplifies switching between providers
```

Quick Notes:

* Base LLMs: Only generate text, no message history. with_structured_output usually does not work on them.
* Chat Models: Designed for multi-turn conversations, structured output parsing, and better context handling.
* init_chat_model: Convenience wrapper; internally returns the appropriate chat model for the provider.


### Messages Roles within the chat interface depending on the message object
* SystemMessage: System role. Instructions the model should use
* HumanMessage: User role. Content "produced" by the user 
* AIMessage: Assistant role. Content generated by the model 
* ChatMessage: message allowing for arbitrary role
* AIMessageChunk: Output of every chunk output when using the stream method.

**Note**: context should be given as human role, not system! (I saw drop in performance otherwise in the boardgames assistant)

### Specific format outputs
* You can force llms to retrieve a specific format output (json, csv, ...), with the help of pydantic.
* You cant use a init_chat_model model. You instead have to use ChatCohere


### Different methods for retrieving outputs
There are 3 methods to generate outputs for all different LangChain objects (models, templates, ...)
* invoke: transforms a single input into an output
* batch: efficiently transforms many inputs into many outputs
* stream: streams output from a single input as it is produced (outputs one token at a time)

Furthermore, **each of the 3 methods have asyncio equivalents!**

### Assembling LangChain objects
There are 2 ways to combine LangChain objects:
* Imperative: you just put together all steps inside a function, and add the @chain decorator to the function.
* Declarative: use "LangChain Expression Language" (LCEL)
The declarative syntax is super easy (you just add a "|") and it offers automatic parallel execution, streaming, and async execution. In the imperative syntax, you have to do that manually. So usually the declarative is easier and also better, but the imperative is useful if you have to write a lot of custom logic (the declarative simply chains existing components, with limited customization).
The cool thing about assembling objects, is that you can still use .invoke, .batch and .stream on the chain (kind of like sklearn pipelines that still have transform, predict and fit methods).


### Extra
* There is a **with_retry** option in the models that could be worth exploring. The book does not cover retries. There are also functionalities for fallbacks.
